Download Link: https://assignmentchef.com/product/solved-coen240-homework5
<br>
You are a robot in a lumber yard, and must learn to discriminate Oak wood from Pine wood. You choose to learn a Decision Tree classifier. You are given the following examples:

<table width="324">

 <tbody>

  <tr>

   <td width="85"><strong>Example </strong></td>

   <td width="63"><strong>Density </strong></td>

   <td width="50"><strong>Grain </strong></td>

   <td width="76"><strong>Hardness </strong></td>

   <td width="50"><strong>Class </strong></td>

  </tr>

  <tr>

   <td width="85">Example #1</td>

   <td width="63">Heavy</td>

   <td width="50">Small</td>

   <td width="76">Hard</td>

   <td width="50">Oak</td>

  </tr>

  <tr>

   <td width="85">Example #2</td>

   <td width="63">Heavy</td>

   <td width="50">Large</td>

   <td width="76">Hard</td>

   <td width="50">Oak</td>

  </tr>

  <tr>

   <td width="85">Example #3</td>

   <td width="63">Heavy</td>

   <td width="50">Small</td>

   <td width="76">Hard</td>

   <td width="50">Oak</td>

  </tr>

  <tr>

   <td width="85">Example #4</td>

   <td width="63">Light</td>

   <td width="50">Large</td>

   <td width="76">Soft</td>

   <td width="50">Oak</td>

  </tr>

  <tr>

   <td width="85">Example #5</td>

   <td width="63">Light</td>

   <td width="50">Large</td>

   <td width="76">Hard</td>

   <td width="50">Pine</td>

  </tr>

  <tr>

   <td width="85">Example #6</td>

   <td width="63">Heavy</td>

   <td width="50">Small</td>

   <td width="76">Soft</td>

   <td width="50">Pine</td>

  </tr>

  <tr>

   <td width="85">Example #7</td>

   <td width="63">Heavy</td>

   <td width="50">Large</td>

   <td width="76">Soft</td>

   <td width="50">Pine</td>

  </tr>

  <tr>

   <td width="85">Example #8</td>

   <td width="63">Heavy</td>

   <td width="50">Small</td>

   <td width="76">Soft</td>

   <td width="50">Pine</td>

  </tr>

 </tbody>

</table>

<strong>1.1</strong> Which attribute will be chosen as the root of the tree (show derivations)?

<strong>1.2 </strong>Derive the complete decision tree by recursively applying the smallest entropy criterion to select root nodes of sub-trees (show derivations). Then draw the complete decision tree.

<strong>Problem 2  </strong>

NASA wants to discriminate Martians (M) from Humans (H) based on these features (attributes): Green ∈ {N, Y}, Legs∈ {2,3}, Height∈ {S, T}, Smelly ∈ {N, Y}. Your available training data is as follows (N=No, Y=Yes, S=Short, T=Tall):

(h) Derive the complete decision tree by recursively applying the smallest entropy criterion to select root nodes of sub-trees (show derivations). Then draw the complete decision tree.




<strong> </strong>

<strong> </strong>

<strong> </strong>

<strong> </strong>

<strong> </strong>

<strong> </strong>

<strong> </strong>

<strong> </strong>

<strong> </strong>

<strong> </strong>

<strong> </strong>

<strong> </strong>

<strong> </strong>

<strong> </strong>

<strong> </strong>

<strong> </strong>

<strong> </strong>

<strong> </strong>

<strong> </strong>